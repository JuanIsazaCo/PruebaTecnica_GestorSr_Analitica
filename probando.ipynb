{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\jisaza53\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\jisaza53\\appdata\\roaming\\python\\python312\\site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jisaza53\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jisaza53\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (4.67.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jisaza53\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jisaza53\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jisaza53\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jisaza53\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\jisaza53\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar las librerías (deben de estar previamente instaladas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import kagglehub\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descargar herramientas para NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jisaza53\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jisaza53\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords (elimina palabras vacias) y WordNetLemmatizer(lematización de palabras en un texto - La lematización es el proceso de reducir una palabra a su forma base o raíz)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descargo todas las tablas de datos de Kaggle (21 G) :P Bastante info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = kagglehub.dataset_download(\"cynthiarempel/amazon-us-customer-reviews-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = rf'C:\\Users\\jisaza53\\OneDrive - Cementos Argos S.A\\Escritorio\\PruebaTecnica_GestorSr\\DataSet\\amazon_reviews_us_Personal_Care_Appliances_v1_00.tsv'\n",
    "try:\n",
    "    df = pd.read_csv(path, sep='\\t',on_bad_lines='skip')\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>32114233</td>\n",
       "      <td>R1QX6706ZWJ1P5</td>\n",
       "      <td>B00OYRW4UE</td>\n",
       "      <td>223980852</td>\n",
       "      <td>Elite Sportz Exercise Sliders are Double Sided...</td>\n",
       "      <td>Personal_Care_Appliances</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Good quality. Shipped</td>\n",
       "      <td>Exactly as described. Good quality. Shipped fast</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>18125776</td>\n",
       "      <td>R3QWMLJHIW6P37</td>\n",
       "      <td>B0000537JQ</td>\n",
       "      <td>819771537</td>\n",
       "      <td>Ezy Dose Weekly</td>\n",
       "      <td>Personal_Care_Appliances</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>It is great</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>19917519</td>\n",
       "      <td>R14Z1VR1N0Z9G6</td>\n",
       "      <td>B00HXXO332</td>\n",
       "      <td>849307176</td>\n",
       "      <td>Pulse Oximeter, Blood Oxygen Monitor</td>\n",
       "      <td>Personal_Care_Appliances</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>It's really nice it works great</td>\n",
       "      <td>It's really nice it works great. You have the ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>18277171</td>\n",
       "      <td>R25ZRJL0GH0U0</td>\n",
       "      <td>B00EOB0JA2</td>\n",
       "      <td>700864740</td>\n",
       "      <td>SE Tools Tool Kit Watch Watch Repair Kit (20 P...</td>\n",
       "      <td>Personal_Care_Appliances</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>The kit works fine... simple cheap plastic tho</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>2593270</td>\n",
       "      <td>R3837KYH7AZNIY</td>\n",
       "      <td>B00OC2O1UC</td>\n",
       "      <td>794298839</td>\n",
       "      <td>doTERRA HD Clear Facial Kit - Facial Lotion, F...</td>\n",
       "      <td>Personal_Care_Appliances</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>It works better than anything else ive tried</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     32114233  R1QX6706ZWJ1P5  B00OYRW4UE       223980852   \n",
       "1          US     18125776  R3QWMLJHIW6P37  B0000537JQ       819771537   \n",
       "2          US     19917519  R14Z1VR1N0Z9G6  B00HXXO332       849307176   \n",
       "3          US     18277171   R25ZRJL0GH0U0  B00EOB0JA2       700864740   \n",
       "4          US      2593270  R3837KYH7AZNIY  B00OC2O1UC       794298839   \n",
       "\n",
       "                                       product_title  \\\n",
       "0  Elite Sportz Exercise Sliders are Double Sided...   \n",
       "1                                    Ezy Dose Weekly   \n",
       "2               Pulse Oximeter, Blood Oxygen Monitor   \n",
       "3  SE Tools Tool Kit Watch Watch Repair Kit (20 P...   \n",
       "4  doTERRA HD Clear Facial Kit - Facial Lotion, F...   \n",
       "\n",
       "           product_category  star_rating  helpful_votes  total_votes vine  \\\n",
       "0  Personal_Care_Appliances            5              0            0    N   \n",
       "1  Personal_Care_Appliances            5              0            0    N   \n",
       "2  Personal_Care_Appliances            5              1            1    N   \n",
       "3  Personal_Care_Appliances            2              0            0    N   \n",
       "4  Personal_Care_Appliances            4              0            1    N   \n",
       "\n",
       "  verified_purchase                  review_headline  \\\n",
       "0                 Y            Good quality. Shipped   \n",
       "1                 Y                       Five Stars   \n",
       "2                 Y  It's really nice it works great   \n",
       "3                 Y                        Two Stars   \n",
       "4                 Y                       Four Stars   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0   Exactly as described. Good quality. Shipped fast  2015-08-31  \n",
       "1                                        It is great  2015-08-31  \n",
       "2  It's really nice it works great. You have the ...  2015-08-31  \n",
       "3     The kit works fine... simple cheap plastic tho  2015-08-31  \n",
       "4       It works better than anything else ive tried  2015-08-31  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizamos la información del  las reseñas (Calidad del dato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_headline</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good quality. Shipped</td>\n",
       "      <td>good quality shipped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>five star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's really nice it works great</td>\n",
       "      <td>really nice work great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two Stars</td>\n",
       "      <td>two star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Four Stars</td>\n",
       "      <td>four star</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   review_headline          cleaned_review\n",
       "0            Good quality. Shipped    good quality shipped\n",
       "1                       Five Stars               five star\n",
       "2  It's really nice it works great  really nice work great\n",
       "3                        Two Stars                two star\n",
       "4                       Four Stars               four star"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocesar el texto stop(stopwords - Elimina los espacios en blanco y WordNetLemmatizer - que corta palabras hasta obtener una raíz )\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):  # Si el texto no es una cadena (e.g., NaN o float), lo convertimos en vacío\n",
    "        return ''\n",
    "    text = text.lower()  # Convertir a minúsculas\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Eliminar caracteres especiales\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Aplicar preprocesamiento a la columna 'review_headline' (puede variar dependiendo del nombre de la columna)\n",
    "df['cleaned_review'] = df['review_headline'].apply(preprocess_text)\n",
    "\n",
    "# Verificar resultados\n",
    "df[['review_headline', 'cleaned_review']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos un dataframe donde se contenga las clasificaciones para analizar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    feeling = pd.read_excel(r'C:\\Users\\jisaza53\\OneDrive - Cementos Argos S.A\\Escritorio\\PruebaTecnica_GestorSr\\DataSet\\Feelings.xlsx',header=0)\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud de df: 85924\n",
      "Longitud de feeling: 119\n"
     ]
    }
   ],
   "source": [
    "# Verifica que ambos DataFrames tengan el mismo número de filas\n",
    "print(f\"Longitud de df: {len(df)}\")\n",
    "print(f\"Longitud de feeling: {len(feeling)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [85924, 119]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m feeling[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Asegúrate de que esta columna existe\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# División en conjunto de entrenamiento y prueba\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Entrenamiento del modelo Naive Bayes\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m MultinomialNB()\n",
      "File \u001b[1;32mc:\\Users\\jisaza53\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jisaza53\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2782\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2782\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2784\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2785\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2786\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2787\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jisaza53\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\jisaza53\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [85924, 119]"
     ]
    }
   ],
   "source": [
    "# Limpiar las reseñas\n",
    "df['cleaned_review'] = df['cleaned_review'].fillna('')  # Lidiar con NaN\n",
    "\n",
    "# Vectorización TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limitar el número de características\n",
    "X = vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Etiquetas (suponiendo que 'sentiment' está en el dataframe de 'feeling')\n",
    "y = feeling['sentiment']  # Asegúrate de que esta columna existe\n",
    "\n",
    "# División en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenamiento del modelo Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Análisis de las palabras clave para las reseñas positivas, negativas y neutras\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_pos = vectorizer.fit_transform(df[df['sentiment'] == 'positive']['cleaned_review'])\n",
    "X_neg = vectorizer.fit_transform(df[df['sentiment'] == 'negative']['cleaned_review'])\n",
    "X_neu = vectorizer.fit_transform(df[df['sentiment'] == 'neutral']['cleaned_review'])\n",
    "\n",
    "# Mostrar las palabras clave más frecuentes en cada clase\n",
    "def top_n_keywords(X, n=10):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    sum_words = X.sum(axis=0)\n",
    "    word_freq = [(word, sum_words[0, idx]) for word, idx in zip(feature_names, range(len(feature_names)))]\n",
    "    sorted_word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_word_freq[:n]\n",
    "\n",
    "print(\"Top palabras positivas:\", top_n_keywords(X_pos))\n",
    "print(\"Top palabras negativas:\", top_n_keywords(X_neg))\n",
    "print(\"Top palabras neutras:\", top_n_keywords(X_neu))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
